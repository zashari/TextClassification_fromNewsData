{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982e6bba-83bc-464b-8bfc-4d2599e2a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d2f9bf-6b86-4746-92ba-114116b65b50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CNN News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8156f873-98b7-4664-a4d2-f5c20d63f388",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Funtion untuk melakukan scraping berita\n",
    "def scrape_news_detail(url):\n",
    "    \n",
    "    # Membuat HTTP request ke website\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Pastikan request berhasil dengan status code 200\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to retrieve the webpage')\n",
    "        return None\n",
    "\n",
    "    # Parse HTML dari website menggunakan BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    content = soup.select_one('.detail-text p').text.strip()\n",
    "    return {\n",
    "        content \n",
    "    }\n",
    "\n",
    "def scrape_news(url):\n",
    "    # Membuat HTTP request ke website\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Pastikan request berhasil dengan status code 200\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to retrieve the webpage')\n",
    "        return None\n",
    "\n",
    "    # Parse HTML dari website menggunakan BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Inisiasi list kosong untuk menyimpan hasil\n",
    "    news_list = []\n",
    "    \n",
    "    # Scrapping...\n",
    "    articles = soup.select('.flex.gap-6 article.flex-grow')\n",
    "    print(len(articles))\n",
    "    for article in articles:\n",
    "        # print(article)\n",
    "        title = article.select_one('h2').text.strip()\n",
    "        content = article.select_one('a').text.strip()\n",
    "        url = article.select_one('a')['href'].strip()\n",
    "        media = 'CNN Indonesia'\n",
    "        label = article.select_one('.text-cnn_red').text.strip()\n",
    "        \n",
    "        news_list.append({\n",
    "            'Judul': title,\n",
    "            'Text': scrape_news_detail(url),\n",
    "            'Media': media,\n",
    "            'Label': label,\n",
    "        })\n",
    "\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "faa7af67-065c-475a-ad03-fb05917479a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "                                               Judul  \\\n",
      "0  Pakar Ungkap Peradaban Manusia yang Paling Ber...   \n",
      "1  Daftar Kuota Internet Medsos Harian Murah, Cek...   \n",
      "2  4 Bukti Meta dan Medsosnya Tak Netral di Konfl...   \n",
      "3  Cara 'Kejam' Asteroid Bikin Dinosaurus Punah P...   \n",
      "\n",
      "                                                Text          Media      Label  \n",
      "0  {Beberapa peradaban seperti Mesir, China, dan ...  CNN Indonesia  Teknologi  \n",
      "1  {Telkomsel, Tri, dan Axis tercatat punya paket...  CNN Indonesia  Teknologi  \n",
      "2  {Meta, perusahaan yang memiliki Facebook, Inst...  CNN Indonesia  Teknologi  \n",
      "3  {Peneliti di Observatorium Kerajaan Belgia men...  CNN Indonesia  Teknologi  \n"
     ]
    }
   ],
   "source": [
    "# URL berita\n",
    "url = 'https://www.cnnindonesia.com/teknologi/indeks/8?date=2023/11/05'\n",
    "\n",
    "# Scrape berita dari URL\n",
    "news_data = scrape_news(url)\n",
    "\n",
    "# Membuat dataframe baru dan simpan sebagai .csv file\n",
    "if news_data:\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df.to_csv('news_data.csv', mode='a', header=not os.path.exists('news_data.csv'), index=False)\n",
    "    print(news_df)\n",
    "else:\n",
    "    print(\"No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbd2867-a148-4f24-9572-7ac64495e263",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CNBC News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b09bb0a-7f1b-4220-bbf6-4e890cea8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Function untuk melakukan scraping berita\n",
    "def scrape_news_detail(url):\n",
    "    \n",
    "    # Membuat HTTP request ke website\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Pastikan request berhasil dengan status code 200\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to retrieve the webpage')\n",
    "        return None\n",
    "\n",
    "    # Parse HTML dari website menggunakan BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    content = soup.select_one('.detail_text p').text.strip()\n",
    "    return {\n",
    "        content \n",
    "    }\n",
    "\n",
    "def scrape_news(url):\n",
    "    # Membuat HTTP request ke website\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Pastikan request berhasil dengan status code 200\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to retrieve the webpage')\n",
    "        return None\n",
    "\n",
    "    # Parse HTML dari website menggunakan BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Inisiasi list kosong untuk menyimpan hasil\n",
    "    news_list = []\n",
    "    \n",
    "   # Scrapping...\n",
    "    articles = soup.select('ul article')[:3]\n",
    "    print(len(articles))\n",
    "    for article in articles:\n",
    "        # print(article)\n",
    "        title = article.select_one('h2').text.strip()\n",
    "        content = article.select_one('a').text.strip()\n",
    "        url = article.select_one('a')['href'].strip()\n",
    "        media = 'CNBC Indonesia'\n",
    "        label = article.select_one('.label').text.strip()\n",
    "        news_list.append({\n",
    "            'Judul': title,\n",
    "            'Text': scrape_news_detail(url),\n",
    "            'Media': media,\n",
    "            'Label': label,\n",
    "        })\n",
    "\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d43c41c8-8e66-48b2-a08e-aff8ea365e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "                                               Judul  \\\n",
      "0  Sudah Terbukti! 2 Kegiatan Ini Bikin Anak Pint...   \n",
      "1  Semangka Jadi Simbol Dukungan ke Palestina, In...   \n",
      "2  Tegas! Perusahaan RI Ini Boikot Produk Penduku...   \n",
      "\n",
      "                                                Text           Media  \\\n",
      "0  {Jakarta, CNBC Indonesia - Otak anak-anak sang...  CNBC Indonesia   \n",
      "1  {Jakarta, CNBC Indonesia - Belakangan emoji se...  CNBC Indonesia   \n",
      "2  {Jakarta, CNBC Indonesia - Sejumlah langkah te...  CNBC Indonesia   \n",
      "\n",
      "       Label  \n",
      "0  Lifestyle  \n",
      "1  Lifestyle  \n",
      "2  Lifestyle  \n"
     ]
    }
   ],
   "source": [
    "# URLberita\n",
    "url = 'https://www.cnbcindonesia.com/lifestyle/indeks/11'\n",
    "\n",
    "# Scrape berita dari URL\n",
    "news_data = scrape_news(url)\n",
    "\n",
    "# membuat Dataframe baru dan simpan sebagai .csv file\n",
    "if news_data:\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df.to_csv('news_data.csv', mode='a', header=not os.path.exists('news_data.csv'), index=False)\n",
    "    print(news_df)\n",
    "else:\n",
    "    print(\"No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80259fe-98c0-4314-81e7-02ad865d56a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Kompas News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23990b65-76ef-425d-a998-c22945bf2360",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Function untuk melakukan scraping berita\n",
    "def scrape_news_detail(url):\n",
    "    \n",
    "    # Membuat HTTP request ke website\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Pastikan request berhasil dengan status code 200\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to retrieve the webpage')\n",
    "        return None\n",
    "\n",
    "    # Parse HTML dari website menggunakan BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    content = soup.select_one('.clearfix p').text.strip()\n",
    "    return {\n",
    "        content \n",
    "    }\n",
    "\n",
    "def scrape_news(url):\n",
    "    # Membuat HTTP request ke website\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Pastikan request berhasil dengan status code 200\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to retrieve the webpage')\n",
    "        return None\n",
    "\n",
    "    # Parse HTML dari website menggunakan BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Inisiasi list kosong untuk menyimpan hasil\n",
    "    news_list = []\n",
    "    \n",
    "    # Scrapping...\n",
    "    articles = soup.select('.article__list')[:3]\n",
    "    print(len(articles))\n",
    "    for article in articles:\n",
    "        # print(article)\n",
    "        title = article.select_one('h3').text.strip()\n",
    "        content = article.select_one('a').text.strip()\n",
    "        url = article.select_one('a')['href'].strip()\n",
    "        media = 'Kompas Indonesia'\n",
    "        # label = extract_label_from_url(url)\n",
    "        label = article.select_one('.article__subtitle').text.strip()\n",
    "        news_list.append({\n",
    "            'Judul': title,\n",
    "            'Text': scrape_news_detail(url),\n",
    "            'Media': media,\n",
    "            'Label': label,\n",
    "        })\n",
    "\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4262f3fa-5729-4497-87e0-368a809d811c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "                                               Judul  \\\n",
      "0  PDI-P Sebut Prabowo-Gibran Neo Orde Baru, Geri...   \n",
      "1  Gerindra: Tidak Ada Istilah Prabowo Kalah di J...   \n",
      "2  Erick Thohir Diduga Merugi Kerja Politiknya Ga...   \n",
      "\n",
      "                                                Text             Media  \\\n",
      "0  {PDI-P Sebut Prabowo-Gibran Neo Orde Baru, Ger...  Kompas Indonesia   \n",
      "1  {Gerindra: Tidak Ada Istilah Prabowo Kalah di ...  Kompas Indonesia   \n",
      "2  {Erick Thohir Diduga Merugi Kerja Politiknya G...  Kompas Indonesia   \n",
      "\n",
      "      Label  \n",
      "0  Nasional  \n",
      "1  Nasional  \n",
      "2  Nasional  \n"
     ]
    }
   ],
   "source": [
    "# URL berita\n",
    "url = 'https://indeks.kompas.com/?site=nasional'\n",
    "\n",
    "# Scrape berita dari URL\n",
    "news_data = scrape_news(url)\n",
    "\n",
    "# membuat datafrane dan simpan sebagai .csv file\n",
    "if news_data:\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df.to_csv('news_data.csv', mode='a', header=not os.path.exists('news_data.csv'), index=False)\n",
    "    print(news_df)\n",
    "else:\n",
    "    print(\"No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea2a4bd-3f61-478b-b12e-3ec452c967ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Suara Edukasi News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7221c3c0-9c56-44d2-868b-069b40ede16a",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Function untuk melakukan scraping berita\n",
    "def scrape_news_detail(url):\n",
    "    \n",
    "    # Membuat HTTP request ke website\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Pastikan request berhasil dengan status code 200\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to retrieve the webpage')\n",
    "        return None\n",
    "\n",
    "    # Parse HTML dari website menggunakan BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    content = soup.select_one('.entry-content p').text.strip()\n",
    "    return {\n",
    "        content \n",
    "    }\n",
    "\n",
    "def scrape_news(url):\n",
    "    # Membuat HTTP request ke website\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Pastikan request berhasil dengan status code 200\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to retrieve the webpage')\n",
    "        return None\n",
    "\n",
    "    # Parse HTML dari website menggunakan BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Inisiasi list kosong untuk menyimpan hasil\n",
    "    news_list = []\n",
    "    \n",
    "    # Scrapping...\n",
    "    articles = soup.select('.news-list-post-wrap article')[:3]\n",
    "    print(len(articles))\n",
    "    for article in articles:\n",
    "        # print(article)\n",
    "        title = article.select_one('h2').text.strip()\n",
    "        content = article.select_one('a').text.strip()\n",
    "        url = article.select_one('a')['href'].strip()\n",
    "        media = 'Suara Edukasi'\n",
    "        # label = article.select_one('.article__subtitle').text.strip()\n",
    "        news_list.append({\n",
    "            'Judul': title,\n",
    "            'Text': scrape_news_detail(url),\n",
    "            'Media': media,\n",
    "            'Label': \"UMKM\",\n",
    "        })\n",
    "\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6e7b584-8c6c-4467-89ea-f1193fed4734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "                                               Judul  \\\n",
      "0  DISKUSI RINGAN TENTANG PEMBERDAYAAN MASYARAKAT...   \n",
      "1  Momentum Hari Ulang Tahun Kemerdekaan RI untuk...   \n",
      "2  HARI ULANG TAHUN UMKM: Mendorong Transformasi ...   \n",
      "\n",
      "                                                Text          Media Label  \n",
      "0  {Luwuk, 25 Agustus 2023, bertempat di salah sa...  Suara Edukasi  UMKM  \n",
      "1                      {Oleh : Budiman Jaya Ashari*}  Suara Edukasi  UMKM  \n",
      "2                      {Oleh : Budiman Jaya Ashari*}  Suara Edukasi  UMKM  \n"
     ]
    }
   ],
   "source": [
    "# URL berita\n",
    "url = 'https://suara-edukasi.com/?s=umkm'\n",
    "\n",
    "# Scrape berita dari URL\n",
    "news_data = scrape_news(url)\n",
    "\n",
    "# Membuat sebuah dataframe dan simpan sebagai .csv file\n",
    "if news_data:\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df.to_csv('news_data.csv', mode='a', header=not os.path.exists('news_data.csv'), index=False)\n",
    "    print(news_df)\n",
    "else:\n",
    "    print(\"No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f65e539-29db-40b4-8624-6f49c6225718",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Folkative News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d5aff92-263b-4806-82a0-8dcb23580a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Function untuk melakukan scraping berita\n",
    "def scrape_news_detail(url):\n",
    "    \n",
    "    # Membuat HTTP request ke website\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Pastikan request berhasil dengan status code 200\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to retrieve the webpage')\n",
    "        return None\n",
    "\n",
    "    # Parse HTML dari website menggunakan BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    content = soup.select_one('.entry-content p').text.strip()\n",
    "    return {\n",
    "        content \n",
    "    }\n",
    "\n",
    "def scrape_news(url):\n",
    "    # Membuat HTTP request ke website\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Pastikan request berhasil dengan status code 200\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to retrieve the webpage')\n",
    "        return None\n",
    "\n",
    "    # Parse HTML dari website menggunakan BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Inisiasi list kosong untuk menyimpan hasil\n",
    "    news_list = []\n",
    "    \n",
    "    # Scrapping...\n",
    "    articles = soup.select('.col-md-8 .blog-post')[:6]\n",
    "    print(len(articles))\n",
    "    for article in articles:\n",
    "        # print(article)\n",
    "        title = article.select_one('h3').text.strip()\n",
    "        content = article.select_one('a').text.strip()\n",
    "        url = article.select_one('a')['href'].strip()\n",
    "        media = 'Folkative'\n",
    "        label = article.select_one('.post-categories').text.strip()\n",
    "        if label == \"Music\":\n",
    "            news_list.append({\n",
    "            'Judul': title,\n",
    "            'Text': scrape_news_detail(url),\n",
    "            'Media': media,\n",
    "            'Label': label,\n",
    "        }) \n",
    "\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b1d04b7-8f73-4c70-b6b0-5f3d0b1fd3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "                                               Judul  \\\n",
      "0                Taylor Swift Bisa Bikin IPK Lo Naik   \n",
      "1  Lakuin 4 Tips Ini Kalau Mau Menang War Tiket C...   \n",
      "2  Keseruan Konser NCT DREAM: The DREAM Show 2 di...   \n",
      "\n",
      "                                                Text      Media  Label  \n",
      "0  {Apa lagi yang sering lo dengerin pas belajar?...  Folkative  Music  \n",
      "1  {Setelah digantungin beberapa waktu, siapa yan...  Folkative  Music  \n",
      "2  {Buat Lo para NCTzen, udah nonton konser NCT D...  Folkative  Music  \n"
     ]
    }
   ],
   "source": [
    "# URL berita\n",
    "url = 'https://folkative.com/category/music/'\n",
    "\n",
    "# Scrape berita dari URL\n",
    "news_data = scrape_news(url)\n",
    "\n",
    "# Membuat sebuah df dan simpan sebaagai .csv file\n",
    "if news_data:\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df.to_csv('news_data.csv', mode='a', header=not os.path.exists('news_data.csv'), index=False)\n",
    "    print(news_df)\n",
    "else:\n",
    "    print(\"No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e855fc7-6400-43c4-abd9-2c77cb603f2d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## BINUS News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf4c96c2-0566-41e1-a3b0-b4c31d59cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Funtion untuk melakukan scraping berita\n",
    "def scrape_news_detail(url):\n",
    "    \n",
    "    # Membuat HTTP request ke website\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Pastikan request berhasil dengan status code 200\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to retrieve the webpage')\n",
    "        return None\n",
    "\n",
    "    # Parse HTML dari website menggunakan BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    content = soup.select_one('.wp-content p').text.strip()\n",
    "    return {\n",
    "        content \n",
    "    }\n",
    "\n",
    "def scrape_news(url):\n",
    "    # Membuat HTTP request ke website\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Pastikan request berhasil dengan status code 200\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to retrieve the webpage')\n",
    "        return None\n",
    "\n",
    "    # Parse HTML dari website menggunakan BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Inisiasi empty list untuk menyimpan hasil\n",
    "    news_list = []\n",
    "    \n",
    "    # Scrapping...\n",
    "    articles = soup.select('ul .the-post')[3:]\n",
    "    print(len(articles))\n",
    "    for article in articles:\n",
    "        # print(article) \n",
    "        title = article.select_one('h3').text.strip()\n",
    "        content = article.select_one('a').text.strip()\n",
    "        url = article.select_one('a')['href'].strip()\n",
    "        media = 'BINUS NEWS'\n",
    "        # label = article.select_one('.label').text.strip()\n",
    "        label = \"BINUS\"\n",
    "        news_list.append({\n",
    "            'Judul': title,\n",
    "            'Text': scrape_news_detail(url),\n",
    "            'Media': media,\n",
    "            'Label': label,\n",
    "        })\n",
    "\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8bab3ee3-f071-4973-8c4d-322ea96a0937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "                                               Judul  \\\n",
      "0             Boarder Gathering “Chill Out Together”   \n",
      "1    Student Development Newsletter - September 2023   \n",
      "2  Training Pembekalan Aktivis BSQ Periode 2023/2024   \n",
      "3                       Welcoming New Housing Member   \n",
      "4      Student Development Newsletter - Agustus 2023   \n",
      "5  Upacara 17 Agustus dan Appreciation Day 2023 d...   \n",
      "\n",
      "                                                Text       Media  Label  \n",
      "0  {Boarder Gathering merupakan kegiatan yang dis...  BINUS NEWS  BINUS  \n",
      "1                                                 {}  BINUS NEWS  BINUS  \n",
      "2                                                 {}  BINUS NEWS  BINUS  \n",
      "3  {Welcoming New Housing Member merupakan kegiat...  BINUS NEWS  BINUS  \n",
      "4                                                 {}  BINUS NEWS  BINUS  \n",
      "5  {17 Agustus 2023, BINUS University @Alam Suter...  BINUS NEWS  BINUS  \n"
     ]
    }
   ],
   "source": [
    "# URL news\n",
    "url = 'https://student.binus.ac.id/category/news/'\n",
    "\n",
    "# Scrape berita dari URL\n",
    "news_data = scrape_news(url)\n",
    "\n",
    "# Scrapping...\n",
    "# membuat Dataframe dan menyimpan Df tersebut sebagai .csv file\n",
    "if news_data:\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df.to_csv('news_data.csv', mode='a', header=not os.path.exists('news_data.csv'), index=False)\n",
    "    print(news_df)\n",
    "else:\n",
    "    print(\"No data scraped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
